{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12708575,"sourceType":"datasetVersion","datasetId":8031989}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:22:50.723146Z","iopub.execute_input":"2025-08-09T12:22:50.723395Z","iopub.status.idle":"2025-08-09T12:22:52.488257Z","shell.execute_reply.started":"2025-08-09T12:22:50.723371Z","shell.execute_reply":"2025-08-09T12:22:52.487569Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/firebaseserviceaccountkey/sahayak2-5ed9b-firebase-adminsdk-fbsvc-0320b2f8a4.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================\n# 1️⃣ Install Required Packages\n# ==============================\n!pip install -q fastapi uvicorn transformers accelerate bitsandbytes pyngrok firebase-admin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:22:52.490053Z","iopub.execute_input":"2025-08-09T12:22:52.490523Z","iopub.status.idle":"2025-08-09T12:24:32.014546Z","shell.execute_reply.started":"2025-08-09T12:22:52.490490Z","shell.execute_reply":"2025-08-09T12:24:32.013553Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==============================\n# 1. Imports and Secrets\n# ==============================\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport uvicorn\nimport threading\nimport time # <--- Import the time library\nfrom kaggle_secrets import UserSecretsClient\n\n# Get secrets from Kaggle\nuser_secrets = UserSecretsClient()\nsecret_hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nsecret_ngrok_token = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n\n# ==============================\n# 2. Model Loading (4-bit)\n# ==============================\nMODEL_PATH = \"shl0k/Sahayak-Llama-3-8B-Instruct-v1\"\n\nprint(\"⏳ Loading Sahayak model...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=secret_hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n    token=secret_hf_token\n)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(\"✅ Sahayak model loaded successfully.\")\n\n# ==============================\n# 3. FastAPI App\n# ==============================\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass Item(BaseModel):\n    prompt: str\n\n@app.post(\"/generate\")\ndef generate_text(item: Item):\n    prompt = item.prompt\n    if not prompt:\n        return {\"response\": \"Error: No prompt provided.\"}\n\n    print(f\"📥 Prompt received: {prompt[:80]}...\")\n    output = pipe(prompt, max_new_tokens=250, temperature=0.7, do_sample=True)\n    model_response = output[0]['generated_text']\n    print(\"✅ Response generated.\")\n    \n    return {\"response\": model_response}\n\n# ==============================\n# 4. Start ngrok & Server\n# ==============================\nngrok.set_auth_token(secret_ngrok_token)\n\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nthread = threading.Thread(target=run_server)\nthread.start()\n\npublic_url = ngrok.connect(8000)\nprint(\"====================================================================\")\nprint(\"🚀 Your Sahayak Model is Live!\")\nprint(f\"🔗 Public API URL: {public_url}\")\nprint(\"====================================================================\")\n\n# --- START OF FIX ---\n# Keep the main script running indefinitely to keep the server alive.\n# This loop will pause the script for 60 seconds at a time, forever.\nwhile True:\n    time.sleep(60)\n# --- END OF FIX ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:24:32.016225Z","iopub.execute_input":"2025-08-09T12:24:32.016527Z","execution_failed":"2025-08-09T14:19:08.994Z"}},"outputs":[{"name":"stderr","text":"2025-08-09 12:24:43.983122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754742284.164763      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754742284.215421      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"⏳ Loading Sahayak model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbebaf0ad2424536a240a5992accb1c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b18e0e922004b248a5ad63e043c3288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c89ae5629c842978df272e43a9259ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfb03fa63c04e85a9ee2792f0f95151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5219033e8df4a568022dc5b285de7d5"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6481e8e533504d31b277887891741899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd721aae835a470e820e410c72b773ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61225abdcef498eb8ce1f252a7945d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d79915ad421437081458b21e47de11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a437e077e9cb486d9ccafb5bcda8d8db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75868142cc4a448191ffef83e5734feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0e9e9e79f5f434ba6629a316d21a266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3ce17f8f674cba9018ac112973f1d2"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"✅ Sahayak model loaded successfully.\n                                                                                                    \r","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"====================================================================\n🚀 Your Sahayak Model is Live!\n🔗 Public API URL: NgrokTunnel: \"https://8c5513f2cb14.ngrok-free.app\" -> \"http://localhost:8000\"\n====================================================================\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"📥 Prompt received: You are Sahayak, an expert AI assistant that creates simple analogies using Indi...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Response generated.\nINFO:     223.185.134.147:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"📥 Prompt received: You are Sahayak, an expert AI assistant that creates simple analogies using Indi...\n✅ Response generated.\nINFO:     223.185.134.147:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"📥 Prompt received: You are Sahayak, an expert AI assistant that creates simple analogies using Indi...\n✅ Response generated.\nINFO:     223.185.134.147:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"📥 Prompt received: You are Sahayak, an expert AI assistant that creates simple analogies using Indi...\n✅ Response generated.\nINFO:     223.185.134.147:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"📥 Prompt received: You are Sahayak, an expert AI assistant that creates simple analogies using Indi...\n✅ Response generated.\nINFO:     223.185.134.147:0 - \"POST /generate HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":null}]}